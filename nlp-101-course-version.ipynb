{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-11-10T19:43:31.777568Z","iopub.status.busy":"2022-11-10T19:43:31.776574Z","iopub.status.idle":"2022-11-10T19:43:31.784167Z","shell.execute_reply":"2022-11-10T19:43:31.782765Z","shell.execute_reply.started":"2022-11-10T19:43:31.77752Z"}},"source":["# Welcome to NLP 101!\n","This notebook will be covering both classical NLP and deep learning NLP techniques.\n","Let's import some packages that we will be using first. We will be first using nltk - the natural language toolkit to show us some classical NLP capabilities. These are important as we can still use these techniques in conjunction with our ML/DL models as preprocessing steps."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:41:48.975718Z","iopub.status.busy":"2022-11-11T01:41:48.975293Z","iopub.status.idle":"2022-11-11T01:41:50.883293Z","shell.execute_reply":"2022-11-11T01:41:50.882124Z","shell.execute_reply.started":"2022-11-11T01:41:48.975684Z"},"trusted":true},"outputs":[],"source":["##bit of change for git practice\n","import nltk\n","nltk.download('omw-1.4')"]},{"cell_type":"markdown","metadata":{},"source":["## A. Classical NLP\n","Let's define an input paragraph that we want to analyze first!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:41:50.887105Z","iopub.status.busy":"2022-11-11T01:41:50.886331Z","iopub.status.idle":"2022-11-11T01:41:50.89319Z","shell.execute_reply":"2022-11-11T01:41:50.892037Z","shell.execute_reply.started":"2022-11-11T01:41:50.887055Z"},"trusted":true},"outputs":[],"source":["# Feel free to change the text - this is from Wikipedia\n","text = \"Tower Bridge is a drawbridge in London. It crosses the River Thames near the Tower of London. It allows ships through the bridge deck when is raised at an angle in the centre. The north side of the bridge is Tower Hill, and the south side of the bridge comes down into Bermondsey, an area in Southwark. Tower Bridge is far more visible than London Bridge, which people often mistake it for. Many tourists go to London to see the Tower Bridge. It has its own exhibition centre in the horizontal walkway. This gives one of the best vantage points in London.\""]},{"cell_type":"markdown","metadata":{},"source":["### 1. Sentence segmentation\n","Split the sentences up!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:41:50.895645Z","iopub.status.busy":"2022-11-11T01:41:50.8947Z","iopub.status.idle":"2022-11-11T01:41:50.924837Z","shell.execute_reply":"2022-11-11T01:41:50.923592Z","shell.execute_reply.started":"2022-11-11T01:41:50.895608Z"},"trusted":true},"outputs":[],"source":["sentences = nltk.sent_tokenize(text)\n","print(sentences)"]},{"cell_type":"markdown","metadata":{},"source":["### 2. Tokenization\n","Split them into individual words! If this is too easy go look up tokenization in Japanese which cannot rely on spaces to split the words. -> Search 'MeCab Japanese'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:41:50.927462Z","iopub.status.busy":"2022-11-11T01:41:50.92631Z","iopub.status.idle":"2022-11-11T01:41:50.933518Z","shell.execute_reply":"2022-11-11T01:41:50.932317Z","shell.execute_reply.started":"2022-11-11T01:41:50.927424Z"},"trusted":true},"outputs":[],"source":["words = nltk.word_tokenize(text)\n","print(words)"]},{"cell_type":"markdown","metadata":{},"source":["Seems like we haven't kept the work we did in the sentence segmentation section - how should we apply word tokenization on top of sentence segmentation? Analysing sentences one at a time is usually preferable to minimize complexity."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:41:50.936893Z","iopub.status.busy":"2022-11-11T01:41:50.936345Z","iopub.status.idle":"2022-11-11T01:41:50.951533Z","shell.execute_reply":"2022-11-11T01:41:50.950276Z","shell.execute_reply.started":"2022-11-11T01:41:50.93686Z"},"trusted":true},"outputs":[],"source":["# Write your code here!"]},{"cell_type":"markdown","metadata":{},"source":["### 3. Part-of-speech tagging\n","Now we want to analyse the first sentence to see what parts of speech are present. Usually we can use just the nouns present to guess what is going on in a sentence."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:41:50.954194Z","iopub.status.busy":"2022-11-11T01:41:50.952876Z","iopub.status.idle":"2022-11-11T01:41:51.123488Z","shell.execute_reply":"2022-11-11T01:41:51.12211Z","shell.execute_reply.started":"2022-11-11T01:41:50.954113Z"},"trusted":true},"outputs":[],"source":["words = nltk.word_tokenize(sentences[0])\n","print(f'First sentence tokenized: {words}')\n","pos_tags = nltk.pos_tag(words)\n","print(f'Part of speech tags: {pos_tags}')"]},{"cell_type":"markdown","metadata":{},"source":["### 4. Normalization\n","We want to reduce vocabulary size which will help improve our NLP.\n","#### 4.1 Stemming\n","Simply truncating words to their stems (may not be words themselves). Let's try 2 different stemmers over some examples. Snowball stemmer is the 'upgraded' version of the Porter stemmer, including the ability for users to not process stopwords as sometimes the conjugated forms may have different meanings, e.g. 'to be' and 'a being'."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:41:51.125517Z","iopub.status.busy":"2022-11-11T01:41:51.125071Z","iopub.status.idle":"2022-11-11T01:41:51.131732Z","shell.execute_reply":"2022-11-11T01:41:51.130469Z","shell.execute_reply.started":"2022-11-11T01:41:51.125474Z"},"trusted":true},"outputs":[],"source":["# Define your own word list\n","word_list = ['being', 'a', 'fairly', 'mischievous', 'cat', 'causing', 'trouble', 'late', 'into', 'the', 'night']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:41:51.134182Z","iopub.status.busy":"2022-11-11T01:41:51.133664Z","iopub.status.idle":"2022-11-11T01:41:51.152358Z","shell.execute_reply":"2022-11-11T01:41:51.151375Z","shell.execute_reply.started":"2022-11-11T01:41:51.134114Z"},"trusted":true},"outputs":[],"source":["porter = nltk.stem.PorterStemmer()\n","snowball = nltk.stem.SnowballStemmer('english', ignore_stopwords = True)\n","\n","print([porter.stem(word) for word in word_list])\n","print([snowball.stem(word) for word in word_list])"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.2 Lemmatisation\n","Another option is to reduce words to their base lemmas to 'standardize' words into their synonyms. Lemmatisation is better usually as it is more informative and uses PoS. Let's run the same sentence we stemmed in our lemmatizer."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:41:51.154007Z","iopub.status.busy":"2022-11-11T01:41:51.153688Z","iopub.status.idle":"2022-11-11T01:41:51.161005Z","shell.execute_reply":"2022-11-11T01:41:51.159937Z","shell.execute_reply.started":"2022-11-11T01:41:51.153979Z"},"trusted":true},"outputs":[],"source":["# Function to convert our PoS tags from one type to another used by wordnet\n","def get_wordnet_pos(treebank_tag):\n","    if treebank_tag.startswith('J'):\n","        return nltk.corpus.wordnet.ADJ\n","    elif treebank_tag.startswith('V'):\n","        return nltk.corpus.wordnet.VERB\n","    elif treebank_tag.startswith('N'):\n","        return nltk.corpus.wordnet.NOUN\n","    elif treebank_tag.startswith('R'):\n","        return nltk.corpus.wordnet.ADV\n","    else:\n","        return None\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:41:51.162669Z","iopub.status.busy":"2022-11-11T01:41:51.162336Z","iopub.status.idle":"2022-11-11T01:41:53.153728Z","shell.execute_reply":"2022-11-11T01:41:53.152749Z","shell.execute_reply.started":"2022-11-11T01:41:51.16264Z"},"trusted":true},"outputs":[],"source":["lemmatizer = nltk.stem.WordNetLemmatizer()\n","pos_tags = nltk.pos_tag(word_list) # we need the part of speech tags for lemmatisation\n","print(pos_tags)\n","\n","lemmatized_words = [lemmatizer.lemmatize(pos_tag[0], get_wordnet_pos(pos_tag[1])) if get_wordnet_pos(pos_tag[1]) else pos_tag[0] for pos_tag in pos_tags]\n","print(lemmatized_words)"]},{"cell_type":"markdown","metadata":{},"source":["### 5. Removing stopwords\n","We remove stopwords to get rid of noise that is usually irrelevant."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:41:53.155895Z","iopub.status.busy":"2022-11-11T01:41:53.155212Z","iopub.status.idle":"2022-11-11T01:41:53.162694Z","shell.execute_reply":"2022-11-11T01:41:53.16169Z","shell.execute_reply.started":"2022-11-11T01:41:53.155857Z"},"trusted":true},"outputs":[],"source":["stop_words = nltk.corpus.stopwords.words('english')\n","without_stop_words = [word for word in lemmatized_words if not word in stop_words]\n","print(without_stop_words)"]},{"cell_type":"markdown","metadata":{},"source":["## B. Regex and Fuzzy Matching\n","Let's import some packages we will be using!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:41:53.164631Z","iopub.status.busy":"2022-11-11T01:41:53.164228Z","iopub.status.idle":"2022-11-11T01:42:09.268479Z","shell.execute_reply":"2022-11-11T01:42:09.266866Z","shell.execute_reply.started":"2022-11-11T01:41:53.164598Z"},"trusted":true},"outputs":[],"source":["import re\n","from fuzzywuzzy import fuzz\n","! pip install pyspellchecker\n","from spellchecker import SpellChecker"]},{"cell_type":"markdown","metadata":{},"source":["### 1. Regex\n","Let's try regular expression matching"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:42:09.271387Z","iopub.status.busy":"2022-11-11T01:42:09.270839Z","iopub.status.idle":"2022-11-11T01:42:09.279223Z","shell.execute_reply":"2022-11-11T01:42:09.277944Z","shell.execute_reply.started":"2022-11-11T01:42:09.271329Z"},"trusted":true},"outputs":[],"source":["print(re.sub('a', 'b', 'abacadabra is a magic spell'))\n","print(re.findall(r'[A-Z]+', 'I am feeling okay but if I am happy I ONLY USE CAPS'))"]},{"cell_type":"markdown","metadata":{},"source":["Okay, your turn. Please Google to look up new expressions! regex101.com is great for trying out expressions too\n","\n","Task 1: fill in the regex expression to match only on ['1YESa', 'aYES3', 'asdkfYES', 'YES']."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:42:09.296102Z","iopub.status.busy":"2022-11-11T01:42:09.295732Z","iopub.status.idle":"2022-11-11T01:42:09.307846Z","shell.execute_reply":"2022-11-11T01:42:09.30639Z","shell.execute_reply.started":"2022-11-11T01:42:09.29607Z"},"trusted":true},"outputs":[],"source":["task_1 = '1YESa 2yesb aYES3 asdkfYES YES yES6'\n","print(re.findall(r'[a-zA-Z0-9]*YES[a-zA-Z0-9]*', task_1))"]},{"cell_type":"markdown","metadata":{},"source":["Task 2: fill in the regex expression to match all 6 e-mail addresses in the string"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:42:09.322273Z","iopub.status.busy":"2022-11-11T01:42:09.32179Z","iopub.status.idle":"2022-11-11T01:42:09.332781Z","shell.execute_reply":"2022-11-11T01:42:09.331216Z","shell.execute_reply.started":"2022-11-11T01:42:09.322228Z"},"trusted":true},"outputs":[],"source":["task_2 = 'Hi Sally, Could you please forward this e-mail to the e-mails in this list please? l&d@deloitte.co.uk, nlp101@deloitte.co.uk and aitesting@deloitte.com, ai101@deloitte.com and ukai@deloitte.com, l&d@deloitte.com Thanks! My instagram handle is @swiftnlp.'\n","print(task_2, '\\n')\n","print(re.findall(r'[A-Za-z0-9&]+\\@[A-Za-z]+(?:\\.[A-Za-z]+)+', task_2))"]},{"cell_type":"markdown","metadata":{},"source":["### 2. Fuzzy Matching\n","\n","Quick method for matching strings that are similar but not exactly the same - great for data cleaning!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:42:09.335741Z","iopub.status.busy":"2022-11-11T01:42:09.335258Z","iopub.status.idle":"2022-11-11T01:42:09.34493Z","shell.execute_reply":"2022-11-11T01:42:09.343726Z","shell.execute_reply.started":"2022-11-11T01:42:09.335697Z"},"trusted":true},"outputs":[],"source":["string1 = 'Online NLP 101 Course for Everyone!'\n","string2 = 'Course in NLP'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:42:09.347829Z","iopub.status.busy":"2022-11-11T01:42:09.346705Z","iopub.status.idle":"2022-11-11T01:42:09.367033Z","shell.execute_reply":"2022-11-11T01:42:09.365557Z","shell.execute_reply.started":"2022-11-11T01:42:09.347781Z"},"trusted":true},"outputs":[],"source":["print(fuzz.ratio(string1, string2))\n","print(fuzz.partial_ratio(string1, string2))\n","print(fuzz.token_sort_ratio(string1, string2))\n","print(fuzz.token_set_ratio(string1, string2))"]},{"cell_type":"markdown","metadata":{},"source":["Another use case for fuzzy matching is simple spell checking using pyspellchecker which relies on Levenshtein distance but also the frequency the word appears in the English language. You can then replace these spelling errors!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:42:09.369364Z","iopub.status.busy":"2022-11-11T01:42:09.368556Z","iopub.status.idle":"2022-11-11T01:42:10.185678Z","shell.execute_reply":"2022-11-11T01:42:10.184248Z","shell.execute_reply.started":"2022-11-11T01:42:09.369325Z"},"trusted":true},"outputs":[],"source":["spell = SpellChecker()\n","\n","# find those words that may be misspelled\n","misspelled = spell.unknown(['something', 'is', 'hapenning', 'here'])\n","\n","for word in misspelled:\n","    # Get the one `most likely` answer\n","    print('most likely candidate: ', spell.correction(word))\n","\n","    # Get a list of `likely` options\n","    print('all likely candidates: ', spell.candidates(word))"]},{"cell_type":"markdown","metadata":{},"source":["## C. ML & DL NLP\n","### 1. Named Entity Recognition\n","Let us use a popular NLP package named Spacy which will also do all the pre-processing/Classic NLP steps above!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:42:10.188475Z","iopub.status.busy":"2022-11-11T01:42:10.187618Z","iopub.status.idle":"2022-11-11T01:42:49.314282Z","shell.execute_reply":"2022-11-11T01:42:49.313214Z","shell.execute_reply.started":"2022-11-11T01:42:10.188428Z"},"trusted":true},"outputs":[],"source":["import spacy\n","from spacy import displacy\n","! pip install contextualSpellCheck\n","import contextualSpellCheck"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:42:49.317046Z","iopub.status.busy":"2022-11-11T01:42:49.315827Z","iopub.status.idle":"2022-11-11T01:42:50.264423Z","shell.execute_reply":"2022-11-11T01:42:50.263235Z","shell.execute_reply.started":"2022-11-11T01:42:49.317Z"},"trusted":true},"outputs":[],"source":["nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(text)\n","\n","displacy.render(doc, style=\"ent\")"]},{"cell_type":"markdown","metadata":{},"source":["### 2. BERT use-case: Contextual spell checking\n","Let's use spacy for contextual spell checking too!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:42:50.26644Z","iopub.status.busy":"2022-11-11T01:42:50.265952Z","iopub.status.idle":"2022-11-11T01:43:47.955882Z","shell.execute_reply":"2022-11-11T01:43:47.954475Z","shell.execute_reply.started":"2022-11-11T01:42:50.266405Z"},"trusted":true},"outputs":[],"source":["contextualSpellCheck.add_to_pipe(nlp)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:43:47.959324Z","iopub.status.busy":"2022-11-11T01:43:47.9589Z","iopub.status.idle":"2022-11-11T01:43:50.242Z","shell.execute_reply":"2022-11-11T01:43:50.241105Z","shell.execute_reply.started":"2022-11-11T01:43:47.95928Z"},"trusted":true},"outputs":[],"source":["doc = nlp('Income was $9.4 milion conpar to the prior year of $2.7 milion.')\n","doc._.outcome_spellCheck"]},{"cell_type":"markdown","metadata":{},"source":["### 3. Sentiment Analysis\n","\n","Let us use NLTK's built-in, pretrained sentiment analyser. It is called VADER. VADER is best suited for social media language - short w/ abbreviations and slang."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T01:43:50.245153Z","iopub.status.busy":"2022-11-11T01:43:50.243606Z","iopub.status.idle":"2022-11-11T01:43:50.275282Z","shell.execute_reply":"2022-11-11T01:43:50.274104Z","shell.execute_reply.started":"2022-11-11T01:43:50.245093Z"},"trusted":true},"outputs":[],"source":["from nltk.sentiment import SentimentIntensityAnalyzer\n","sia = SentimentIntensityAnalyzer()\n","sia.polarity_scores(\"Wow, NLTK is really powerful!\")"]},{"cell_type":"markdown","metadata":{},"source":["## D. Build your own ML model for classification"]},{"cell_type":"markdown","metadata":{},"source":["Import all required packages"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2022-11-11T02:44:41.031236Z","iopub.status.busy":"2022-11-11T02:44:41.030797Z","iopub.status.idle":"2022-11-11T02:44:55.722553Z","shell.execute_reply":"2022-11-11T02:44:55.721007Z","shell.execute_reply.started":"2022-11-11T02:44:41.031199Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["!pip install contractions\n","import contractions\n","import string\n","from itertools import chain\n","from nltk.corpus import movie_reviews as mr\n","import pandas as pd\n","import sklearn\n","from sklearn import naive_bayes, linear_model, ensemble, svm"]},{"cell_type":"markdown","metadata":{},"source":["Load in data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T02:14:22.547671Z","iopub.status.busy":"2022-11-11T02:14:22.547276Z","iopub.status.idle":"2022-11-11T02:14:22.71269Z","shell.execute_reply":"2022-11-11T02:14:22.711791Z","shell.execute_reply.started":"2022-11-11T02:14:22.547639Z"},"trusted":true},"outputs":[],"source":["positive_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n","negative_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n","pos_revs = pd.DataFrame([nltk.corpus.movie_reviews.raw(rev) for rev in positive_review_ids], columns = ['review'])\n","neg_revs = pd.DataFrame([nltk.corpus.movie_reviews.raw(rev) for rev in negative_review_ids], columns = ['review'])\n","pos_revs['positive_sentiment'] = 1\n","neg_revs['positive_sentiment'] = 0\n","all_revs = pos_revs.append(neg_revs)\n","all_revs.head(5)"]},{"cell_type":"markdown","metadata":{},"source":["Apply cleaning steps here"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T02:56:27.382444Z","iopub.status.busy":"2022-11-11T02:56:27.382031Z","iopub.status.idle":"2022-11-11T02:57:42.482842Z","shell.execute_reply":"2022-11-11T02:57:42.481934Z","shell.execute_reply.started":"2022-11-11T02:56:27.382413Z"},"trusted":true},"outputs":[],"source":["# Write cleaning code here"]},{"cell_type":"markdown","metadata":{},"source":["Split into train and test set for model training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T02:14:26.726028Z","iopub.status.busy":"2022-11-11T02:14:26.725553Z","iopub.status.idle":"2022-11-11T02:14:26.734065Z","shell.execute_reply":"2022-11-11T02:14:26.732813Z","shell.execute_reply.started":"2022-11-11T02:14:26.725992Z"},"trusted":true},"outputs":[],"source":["train_x, valid_x, train_y, valid_y = sklearn.model_selection.train_test_split(all_revs['review'], all_revs['positive_sentiment'])"]},{"cell_type":"markdown","metadata":{},"source":["Vectorize using 2 methods: count vectorization and tfidf vectorization"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T02:14:28.465451Z","iopub.status.busy":"2022-11-11T02:14:28.465014Z","iopub.status.idle":"2022-11-11T02:14:32.501619Z","shell.execute_reply":"2022-11-11T02:14:32.500404Z","shell.execute_reply.started":"2022-11-11T02:14:28.465419Z"},"trusted":true},"outputs":[],"source":["count_vect = sklearn.feature_extraction.text.CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n","count_vect.fit(all_revs['review'])\n","tfidf_vect = sklearn.feature_extraction.text.TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features = 500)\n","tfidf_vect.fit(all_revs['review'])\n","\n","xtrain_count = count_vect.transform(train_x)\n","xvalid_count = count_vect.transform(valid_x)\n","\n","xtrain_tfidf = tfidf_vect.transform(train_x)\n","xvalid_tfidf = tfidf_vect.transform(valid_x)"]},{"cell_type":"markdown","metadata":{},"source":["Let's try a couple of classic classification models:\n","1. Naive Bayes\n","2. Linear Model (Logistic Regression)\n","3. Support Vector Machines\n","4. Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T02:19:35.983626Z","iopub.status.busy":"2022-11-11T02:19:35.983194Z","iopub.status.idle":"2022-11-11T02:19:35.989845Z","shell.execute_reply":"2022-11-11T02:19:35.988284Z","shell.execute_reply.started":"2022-11-11T02:19:35.983592Z"},"trusted":true},"outputs":[],"source":["# Function created to make it easier to train various models/algorithms\n","def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n","    classifier.fit(feature_vector_train, label)\n","    predictions = classifier.predict(feature_vector_valid)\n","    return sklearn.metrics.classification_report(valid_y, predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-11T02:28:10.38498Z","iopub.status.busy":"2022-11-11T02:28:10.38458Z","iopub.status.idle":"2022-11-11T02:28:31.415796Z","shell.execute_reply":"2022-11-11T02:28:31.414527Z","shell.execute_reply.started":"2022-11-11T02:28:10.384949Z"},"trusted":true},"outputs":[],"source":["nb_cv = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n","print(\"Naive Bayes + Count Vectors: \", nb_cv)\n","\n","nb_tfidf = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n","print(\"Naive Bayes + TFIDF Vectors: \", nb_tfidf)\n","\n","lr_cv = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n","print(\"Logistic Regression + Count Vectors: \", lr_cv)\n","\n","lr_tfidf = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n","print(\"Logistic Regression + TFIDF Vectors: \", lr_tfidf)\n","\n","svm_cv = train_model(svm.SVC(), xtrain_count, train_y, xvalid_count)\n","print(\"Support Vector Machines + Count Vectors: \", svm_cv)\n","\n","svm_tfidf = train_model(svm.SVC(), xtrain_tfidf, train_y, xvalid_tfidf)\n","print(\"Support Vector Machines + TFIDF Vectors: \", svm_tfidf)\n","\n","rf_cv = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n","print(\"Random Forest + Count Vectors: \", rf_cv)\n","\n","rf_tfidf = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n","print(\"Random Forest + TFIDF Vectors: \", rf_tfidf)"]},{"cell_type":"markdown","metadata":{},"source":["The challenge is now to improve the sentiment predictor. There are many methods but the easiest one is by preprocessing the text.\n","\n","If you don't know where to start, try:\n","1. Removing contractions\n","2. Removing stopwords\n","3. Lemmatising the text\n","\n","After your text is clean and processed, you can then start looking at adding additional features (e.g. word count), optimizing the vectorization methods and classification algorithms."]},{"cell_type":"markdown","metadata":{},"source":["## Thank you!\n","Thank you for joining NLP 101 and I hope you have learnt something useful today! Please feel free to contact Michelle (miclai@deloitte.co.uk) if you have any further questions :)\n","\n","Here is how someone else built their own sentiment model with the same dataset:\n","https://realpython.com/python-nltk-sentiment-analysis/#customizing-nltks-sentiment-analysis\n","\n","If you want to try build something more sophisticated, here is a good example with a Kaggle dataset:\n","https://mlwhiz.com/blog/2020/05/24/multiteexamplextclass/"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
